---
title: "Data Journalism Lesson 10: Cleaning text"
output: 
  learnr::tutorial:
    progressive: true
    allow_skip: true
    theme: journal
    ace_theme: github
runtime: shiny_prerendered
description: >
  Clean up your data with text cleaning.
---
```{r setup, include=FALSE}
library(learnr)
library(gradethis)
library(tidyverse)
library(stringr)
library(refinr)
library(glue)
knitr::opts_chunk$set(echo = FALSE)
tutorial_options(exercise.completion=FALSE)
```
```{r load-data, message=FALSE, warning=FALSE}
crashes <- read_csv("https://the-art-of-data-journalism.github.io/tutorial-data/plane-crashes-cleaning/crashes-for-cleaning.csv")

totalcities <-crashes |> 
  group_by(City, State) |> 
  tally()

cleancities <- crashes |> 
  mutate(clean_city = str_to_title(City)) |>
  group_by(clean_city, State) |> 
  tally()

titlechange <- nrow(totalcities) - nrow(cleancities)

keycrashes <- crashes |>
  mutate(
    clean_city = key_collision_merge(City)
    ) |>
  group_by(clean_city, State) |>
  tally() |>
  nrow()

keychange <- nrow(cleancities) - keycrashes

totalkeychanges <- crashes |>
  mutate(
    clean_city = key_collision_merge(City)
    ) |>
  filter(City != clean_city) |> 
  group_by(City, clean_city) |>
  tally() |> 
  nrow()
```
## The Goal

In this lesson, you'll learn about advanced data cleaning techniques using text-cleaning techniques and the R package refinr. By the end of this tutorial, you'll understand how to use some text manipulation functions to merge similar text entries. You'll practice using different techniques and learn to critically evaluate the results. This skill is crucial for data journalists who often work with messy, real-world datasets that require careful standardization before analysis.

## What is Data Journalism?

Gather 'round kids and let me tell you a tale about your author. In college, your author got involved in a project where he mapped crime in the city, looking specifically in the neighborhoods surrounding campus. This was in the mid 1990s. Computers were under powered. Tools were pretty primitive. I was given a database of nearly 50,000 calls for service. 

And then I learned that addresses were not stored in a standard way. However the officer wrote it down, that's how it was recorded. 

What did that mean? 

It meant the Lincoln Police Department came up with dozens of ways to say a single place. And since the mapping software needed the addressed to be in a specific form, I had to fix them. For example, I will go to my grave knowing that Lincoln High School's street address is 2229 J Street. Police officers wrote down LHS, L.H.S., Lincoln HS, Lincoln H.S., LHS (J Street), 2229 J, 2229 J ST, St., Street and on and on and on. That one was relatively easy. A local convenience store chain, with 8 locations around the city, was harder. I had to use the patrol district to locate them. 

It took me four months to clean up more than 30,000 unique addresses and map them. 

I did most of it by hand -- pulling up records in Excel and hand editing them. It was miserable.

What you're going to learn today are things I wish I knew then. Once you know the basics, outside of this tutorial you should look into learning how to use OpenRefine. OpenRefine is a series of tools -- algorithms -- that find small differences in text and helps you fix them quickly. How OpenRefine finds those small differences is through something called clustering. Clustering is where, through algorithms, it finds small meaningless differences between two rows of data -- "Omaha" vs "omaha", for example -- and surmises that they are the same thing.  

I tell you this because if I had OpenRefine, it would have taken me a week, not four months. Every time I talk about OpenRefine, I remember this, and I get mad. 

## The Basics

Data cleaning is a series of logic puzzles. The logic is very simple: I have a goal. Most often that goal is to have all the like things together. The problem is always that someone did something to prevent that. The logic game you are playing is doing things to your text until all the like things match. No matter what it is you need done to the text, someone, somewhere has screwed that up before and a programmer wrote a function to fix it.

Let's dig into some of them, using a version of the plane crash data we've worked with before. This time it's a special cut of the data -- it's national data, but a limited number of columns. It just makes it easier on everyone to have a smaller dataset. 

We're going to start, like every tutorial, with libraries. In this one, we're going to use two new libraries -- stringr and refinr. You should already have them, but if you don't, you can always install packages by going to the console and running `install.packages("package name here")`. The `refinr` package uses some of the same tools as OpenRefine, but with less supervision than OpenRefine does, which limits what you can do with it. But we'll get into that later. 

For now, we're going to solve some easy problems using tools that work with text. 

```{r load-tidyverse, exercise=TRUE}
library(tidyverse)
library(stringr)
library(refinr)
```
```{r load-tidyverse-solution}
library(tidyverse)
library(stringr)
library(refinr)
```
```{r load-tidyverse-check}
grade_this_code()
```

Now we need data. Because this is national data, there's no state name to fill in. You can just run this code chunk.

```{r load-data-exercise, exercise = TRUE}
crashes <- read_csv("https://the-art-of-data-journalism.github.io/tutorial-data/plane-crashes-cleaning/crashes-for-cleaning.csv")
```
```{r load-data-exercise-solution}
crashes <- read_csv("https://the-art-of-data-journalism.github.io/tutorial-data/plane-crashes-cleaning/crashes-for-cleaning.csv")
```
```{r load-data-exercise-check}
grade_this_code()
```

This is the same data from the National Transportation Safety Board of airplane crash investigations since 2019. 

Let's take a quick look at it.

```{r head-data, exercise=TRUE, exercise.setup = "load-data"}
head(____)
```
```{r head-data-solution, exercise.reveal_solution = FALSE}
head(crashes)
```
```{r head-data-check}
grade_this_code()
```

We're specifically going to look at city names. In Data Smells, we learned that we need to look carefully at the data we're going to be relying on. How good of a job has the NTSB done in writing in city names in their database? Quick way? Let's do a simple group by and tally.

```{r group-data, exercise=TRUE, exercise.setup = "load-data"}
crashes |> 
  group_by(City, State) |> 
  tally()
```
```{r group-data-solution, exercise.reveal_solution = FALSE}
crashes |> 
  group_by(City, State) |> 
  tally()
```
```{r group-data-check}
grade_this_code()
```

```{r results, exercise=FALSE, exercise.eval=TRUE, exercise.setup = "load-data", results='asis'}
glue("Doing this, you get {nrow(totalcities)} unique combinations of City and State. Does that mean there's that many different places where plane crashes happened? Without leaving the first page -- the first few results -- should show you the first problem. Notice how the first few rows of cities are all capitalized? Notice one of them is Atlanta, Georgia, home of the busiest airport in the world? What do you want to bet there's a not-all-caps version of Atlanta, Georgia in the data? Don't take that bet. Of course there is.")
```

### Exercise 1: Casing

How do we fix this? Well, you could go through and hand edit the first few cities for every letter of the alphabet, but one thing to keep in mind is that a programmer has already had to deal with this and wrote a function. 

The `stringr` library has a *lot* of functions in it. But among the more useful are the functions that change text to a specific case. For example, want all upppercase letters? There's `str_to_upper()` that does that. And if you guessed there's `str_to_lower()` that does the opposite, good on you, mate. You're right. 

Neither of those solves our problem. What we need is `str_to_title()` which converts things to title case -- aka the first letter is capitalized. Now, is this a perfect solution? Not if you have a city called McDonalds or something like that. MCDONALDS will get turned into Mcdonalds, not McDonalds, using `str_to_title`. But chances are, you have very few of those. And if you have a few, you can fix the vast majority with `str_to_title` and then clean up the few remaining later.

How do we use `str_to_title()`? As a rule, we don't want to overwrite original data, so we're going to use mutate to create a new column and populate it with our new data. So let's make clean_city and use `str_to_title()` making sure to tell that function what column we're changing to title case. Hint: ATLANTA is our problem. What column did that come from?

```{r titlecase, exercise=TRUE, exercise.setup = "load-data"}
crashes |> 
  mutate(____ = ____(____)) |>
  group_by(clean_city, State) |> 
  tally()
```
```{r titlecase-solution, exercise.reveal_solution = FALSE}
crashes |> 
  mutate(clean_city = str_to_title(City)) |>
  group_by(clean_city, State) |> 
  tally()
```
```{r titlecase-check}
grade_this_code()
```

```{r results2, exercise=FALSE, exercise.eval=TRUE, exercise.setup = "load-data", results='asis'}
glue("Doing this, you now get {nrow(cleancities)} unique combinations of City and State. That means we changed the case on cities, then grouped and tallied them and {titlechange} cities melded into their properly-cased home.")
```

### Exercise 2: Misspellings, one at a time

If the data gods are smiling upon you, you'll have no spelling errors in your data. It is a good day when they do smile on you, but today is not that day. You can go diving in and find this if you want, or you can trust me that the NTSB misspelled Albuquerque, New Mexico. No shame -- I had to look it up -- but one time in this data, it's Alburquerque, not Albuquerque. 

If you only have one of those, it's pretty easy to fix with a search and replace. How do you do that in R? With a function called `gsub`, which takes a pretty simple and logical input: What are you looking for? That's first. What do you want to replace it with? That's second. Where can `gsub` find this pattern? Which column? That's third. 

We're going to keep editing `clean_city` until we get what we need. Doing that means we can just chain our new fix onto the previous one. Remember: we're looking for Alburquerque and we want to replace it with Albuquerque. And what column are we going to find that misspelling? It's a bit of a trick question -- remember we're trying to work on a new column called `clean_city` that is the home of all of our changes.

```{r gsub, exercise=TRUE, exercise.setup = "load-data"}
crashes |> 
  mutate(____ = ____(____)) |>
  mutate(clean_city = ____("____", "____", ____)) |> 
  group_by(clean_city) |> 
  tally()
```
```{r gsub-solution, exercise.reveal_solution = FALSE}
crashes |> 
  mutate(clean_city = str_to_title(City)) |>
  mutate(clean_city = gsub("Alburquerque", "Albuquerque", clean_city)) |> 
  group_by(clean_city) |> 
  tally()
```
```{r gsub-check}
grade_this_code()
```

And just like that, our count goes down by one because we fixed Alburquerque ... er, Albuquerque.

Note: You can copy that gsub line and just keep adding as many as you have typos. For example: In Nebraska, there's a city named Fremont that is spelled by the NTSB "Freemont" that you could fix by just copying that line, pasting it in below and swapping out the city names. Rinse and repeat as necessary, if you follow the wisdom of a shampoo bottle.

## Refinr, OpenRefine in R

OpenRefine, as we've discussed, uses clustering algorithms to find things -- OpenRefine calls them facets -- that really are the same thing, but there's some very slight difference between them. In OpenRefine, you have to look at them and approve them, which is a strength that can be very time consuming. 

With the `refinr` package, developers have brought some of those clustering algorithms to R. The upside: very fast and can solve more complex problems than just casing and individual spelling fixes. The downside: you do not get to approve each change, so you can't use your human judgement to decide if Freemont is really Fremont. The algorithm just makes the change.

That limits the applicability of this. Basically, you need to know the problems and you need to know what the solutions look like so you can evaluate them afterwards. 

<div class="alert alert-danger">
<h4 class="alert-heading">Warning</h4> 
<p class="mb-0">Never trust an algorithm to get it 100 percent right. You are responsible for everything you publish. Always check the output.</p>
</div>

The algorithm we're going to use is called `key_collision_merge`. What it does is it looks at what you've given it -- the data -- and creates a different version of each thing in your column containing only the most important parts called the key. Then, it merges those keys together, even if they might be slightly different. 

It's my experience that `key_collision_merge` is the most conservative of the clustering algorithms in this world of data cleaning. It's right far more than it's wrong -- not even close. Some algorithsm you can work with are *really* stretching to match things together. Not so with `key_collision_merge`. 

First, we're going to implement it, then we're going to check it. 

So how do you implement it? Almost exactly the same way we implemented the other two text cleaning techniques in this tutorial. We mutate a new column called clean_city and give it the function. The function works the same way we use other functions -- all `key_collision_merge` needs to know is what column we're giving it to work with. 

### Exercise 3: Implement key_collision_merge

```{r keycollision, exercise=TRUE, exercise.setup = "load-data"}
crashes |>
  mutate(
    clean_city = key_collision_merge(____)
    ) |>
  group_by(clean_city, State) |>
  tally()
```
```{r keycollision-solution, exercise.reveal_solution = FALSE}
crashes |>
  mutate(
    clean_city = key_collision_merge(City)
    ) |>
  group_by(clean_city, State) |>
  tally()
```
```{r keycollision-check}
grade_this_code()
```

```{r results3, exercise=FALSE, exercise.eval=TRUE, exercise.setup = "load-data", results='asis'}
glue("With `key_collision_merge`, we get {keycrashes} unique combinations of `clean_city` and State. Earlier, when we just changed the case, we got {nrow(cleancities)} unique combinations. So we've ... improved? ... by {keychange}. But have we?")
```

### Exercise 4: Checking your work

The way to check your work is to look at where `key_collision_merge` made changes and compare them. We can do that with a simple does not equal filter. We'll know `key_collision_merge` made a change when City doesn't equal clean_city. We can see how many times it made that decision with a group by and tally.

```{r keycollision2, exercise=TRUE, exercise.setup = "load-data"}
crashes |>
  mutate(
    clean_city = key_collision_merge(____)
    ) |>
  filter(____ != ____) |> 
  group_by(City, clean_city) |>
  tally()
```
```{r keycollision2-solution, exercise.reveal_solution = FALSE}
crashes |>
  mutate(
    clean_city = key_collision_merge(City)
    ) |>
  filter(City != clean_city) |> 
  group_by(City, clean_city) |>
  tally()
```
```{r keycollision2-check}
grade_this_code()
```

```{r results4, exercise=FALSE, exercise.eval=TRUE, exercise.setup = "load-data", results='asis'}
glue("If you count up the rows, `key_collision_merge` made {totalkeychanges} changes, but scroll through them. Would you have made the same change? Note how many times `key_collision_merge` chose the all caps version of a city. Or if you scroll down, the bane of any city matching exercise are cities that start with Saint. Is it Saint? St. with the period? St without the period? If you look, `key_collision_merge` isn't consistent with this. So how might you fix that?")
```

The truth is all text-cleaning jobs are unique. Every dataset has their own problems and you have to solve them. Think of this skill like learning how to play music. I've shown you a few chords. Every text-cleaning job is just playing the chords in a different order, a different combination to solve the problem. And there's a lot more chords out there to play. 

How do you fix this city matching problem better? Combining the chords. I'd fix the case first, then change all the instances of "St " -- that's Saint SPACE -- and replace them with "St. ", Saint PERIOD SPACE. That gives `key_collision_merge` less to fuss about. In fact, if you do those two things before giving it to `key_collision_merge`, the total changes drops to 7. All of them involving apostrophes or dashes. 

So can we trust automated data cleaning? 

This note from the documentation is exceedingly important:

> This package is NOT meant to replace OpenRefine for every use case. For situations in which merging accuracy is the most important consideration, OpenRefine is preferable. Since the merging steps in refinr are automated, there will usually be more false positive merges, versus manually selecting clusters to merge in OpenRefine.

In short, only use Refinr where the use case is simple and the changes of a mistake are low and easy to catch if it happens. In cases where you need to do massive corrections of data, OpenRefine with a human in the loop is the better choice. 

## The Recap

Throughout this lesson, you've explored powerful tools for cleaning and standardizing messy data. You've learned how to use `stringr` and `gsub` to make changes to text throughout a dataset, and how to achieve similar results programmatically using the `refinr` package in R. You've practiced different merging techniques and seen how they can dramatically reduce the number of unique entries in a dataset. However, you've also learned the critical importance of verifying the results of automated cleaning processes. Remember, while these tools can greatly speed up your data cleaning workflow, they should be used judiciously and always with careful human oversight to ensure the accuracy and integrity of your data.

## Terms to Know

- OpenRefine: A standalone application for data cleaning that uses clustering algorithms to identify similar text entries.
- Clustering: A technique used to group similar data points together based on certain criteria or algorithms.
- `key_collision_merge`: A method that extracts key parts of strings to identify and merge similar entries.
- `refinr`: An R package that implements OpenRefine's clustering algorithms for use within R scripts.
